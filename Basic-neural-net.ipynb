{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epoch: \n",
    "One epoch consists of one full training cycle on the training set. Once every sample in the set is seen, you start again - marking the beginning of the 2nd epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoches = 10000  #No of iterations\n",
    "inputLayerSize=2\n",
    "hiddenLayerSize=4\n",
    "outputLayerSize=1\n",
    "alpha = 0.1 #learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1,1],[1,2],[2,1],[2,2]])   #input data\n",
    "Y = np.array([[0],[1],[2],[3]])            #output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):                         #activation function\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):            #gradient descent\n",
    "    return x*(1-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wh = np.random.uniform(size=(inputLayerSize,hiddenLayerSize)) #weights used btwn input and hidden layer\n",
    "Wz = np.random.uniform(size=(hiddenLayerSize,outputLayerSize)) #weights used btwn hidden and output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.17654678]\n",
      " [2.32612864]\n",
      " [2.47571744]\n",
      " [2.58035145]]\n"
     ]
    }
   ],
   "source": [
    "H = sigmoid(np.dot(X, Wh))                  # hidden layer results\n",
    "Z = np.dot(H,Wz)                            # output layer, no activation\n",
    "E = Y - Z                                   # how much we missed \n",
    "dZ = E * alpha                               # delta Z\n",
    "Wz +=  H.T.dot(dZ)                          # update output layer weights\n",
    "dH = dZ.dot(Wz.T) * sigmoid_derivative(H)   # delta H\n",
    "Wh +=  X.T.dot(dH)                          # update hidden layer weights\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "Here, X is the input values. Y is the expected output and Z is the output we get.\n",
    "\n",
    "An activation function corresponds to the biological phenomenon of a neuron ‘firing’, i.e. triggering a nerve signal when the neuron’s inputs combine in some appropriate way. It has to be chosen so as to cause reasonably proportionate outputs within a small range, for small changes of input. We’ll use the very popular sigmoid function, but note that there are others. We also need the sigmoid derivative for backpropagation.\n",
    "\n",
    "We’ll make an initial guess using the random initial weights, propagate it through the hidden layer as the dot product of those weights and the input vector of truth-value pairs. Recall that a matrix – vector multiplication proceeds along each row, multiplying each element by corresponding elements down through the vector, and then summing them. This matrix  goes into the sigmoid function to produce H. So H = sigmoid(X * Wh)\n",
    "\n",
    "Same for the Z (output) layer, Z = sigmoid(H * Wz)\n",
    "\n",
    "Now we compare the guess with the training date, i.e. Y – Z, giving E.\n",
    "\n",
    "Finally, backpropagation. This comprises computing changes (deltas) which are multiplied (specifically, via the dot product) with the values at the hidden and input layers, to provide increments for the appropriate weights. If any neuron values are zero or very close, then they aren’t contributing much and might as well not be there. The sigmoid derivative (greatest at zero) used in the backprop will help to push values away from zero. The sigmoid activation function shapes the output at each layer.\n",
    "\n",
    "E is the final error Y – Z.\n",
    "\n",
    "dZ is a change factor dependent on this error magnified by the slope of Z; if its steep we need to change more, if close to zero, not \n",
    "much. The slope is sigmoid_derivative(Z).\n",
    "\n",
    "dH is dZ backpropagated through the weights Wz, amplified by the slope of H.\n",
    "\n",
    "Finally, Wz and Wh are adjusted applying those deltas to the inputs at their layers, because the larger they are, the more the weights need to be changed to absorb the effect of the next forward prop. The input values are the value of the gradient that is being descended; we’re moving the weights down towards the minimum value of the cost function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
